{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hashing vectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "скачайте и распакуйте датасет [Sentiment140 dataset with 1.6 million tweets](https://www.kaggle.com/kazanova/sentiment140) с kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_dataset = \"/Users/d.parpulov/Downloads/training.1600000.processed.noemoticon.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600000, 6)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(path_to_dataset,\n",
    "                      names = ['sentiment', 'id', 'date', 'flag', 'user', 'text'],\n",
    "                      header=None, engine='python')\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  sentiment\n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...          0\n",
       "1  is upset that he can't update his Facebook by ...          0\n",
       "2  @Kenichan I dived many times for the ball. Man...          0\n",
       "3    my whole body feels itchy and like its on fire           0\n",
       "4  @nationwideclass no, it's not behaving at all....          0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset[[\"text\", \"sentiment\"]]\n",
    "dataset[\"sentiment\"] = dataset[\"sentiment\"].apply(lambda x: 1 if x==4 else x)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.unique(dataset[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment\n",
    "- 0 = negative,\n",
    "- 1 = positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1280000,), (320000,), (1280000,), (320000,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dataset[\"text\"], dataset[\"sentiment\"], \n",
    "                                                    test_size=0.2, random_state=42)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Зададим токенизатор\n",
    "можно использовать и дефолтный, но для наглядности мы используем кастомный и применим его позже для Hashing Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text, token_pattern=\"(?:\\w|\\')+\", thresh=1):\n",
    "    text = text.lower()\n",
    "    text = re.sub(\"https?:/\\S+\", '', text)  # remove urls\n",
    "    tokens = re.findall(token_pattern, text)  # split tokens\n",
    "    return filter(lambda x: len(x) > thresh, tokens)  # filter short tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\n",
      "\n",
      "tokens: ['switchfoot', 'awww', \"that's\", 'bummer', 'you', 'shoulda', 'got', 'david', 'carr', 'of', 'third', 'day', 'to', 'do', 'it']\n"
     ]
    }
   ],
   "source": [
    "test_text = dataset[\"text\"].loc[0]\n",
    "print('text:', test_text, end='\\n\\n')\n",
    "print('tokens:', list(tokenizer(test_text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Используем обычный TfidfVectorizer с нашим токенизатором и дефолтными параметрами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.8 s, sys: 314 ms, total: 26.1 s\n",
      "Wall time: 26.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vectorizer = TfidfVectorizer(tokenizer=tokenizer)\n",
    "X_train_vect = vectorizer.fit_transform(X_train)\n",
    "X_test_vect = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1280000, 560343), (320000, 560343))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vect.shape, X_test_vect.shape  # 560k признаков! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучим SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LinearSVC(penalty=\"l2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.795\n",
      "Recall: 0.802\n",
      "F1: 0.798\n",
      "CPU times: user 27.2 s, sys: 319 ms, total: 27.5 s\n",
      "Wall time: 29.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf.fit(X_train_vect, y_train)\n",
    "predicted = clf.predict(X_test_vect)\n",
    "\n",
    "print(\"Precision: %.3f\" % precision_score(y_test, predicted))\n",
    "print(\"Recall: %.3f\" % recall_score(y_test, predicted))\n",
    "print(\"F1: %.3f\" % f1_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полный набор из 560к признаков дает качество из коробки F1=0.798"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Похешируем фичи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hasher(token):\n",
    "    hashed_feature = int(hashlib.sha1(token.encode('utf-8')).hexdigest(), 16)\n",
    "    return hashed_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "493129059604241748898866719106155948524683866694"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"switchfoot\"  # исходный токен\n",
    "hasher(s)  # хешированный токен"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хеши длинные и получаются уникальными для каждого токена. \n",
    "\n",
    "Чтобы сделать набор новых признаков поменьше, уменьшим количество бакетов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hasher(token, n_buckets=10000):\n",
    "    hashed_feature = int(hashlib.sha1(token.encode('utf-8')).hexdigest(), 16)\n",
    "    hashed_feature %= n_buckets\n",
    "    return hashed_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6694"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"switchfoot\"  # исходный токен\n",
    "hasher(s)  # хешированный токен, лежит в диапазоне 0-10k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashed_tokenizer(text, n_buckets=10000, token_pattern='\\w+', thresh=2):\n",
    "    text = text.lower()\n",
    "    text = re.sub(\"https?:/\\S+\", '', text)  # remove urls\n",
    "    tokens = re.findall(token_pattern, text)  # split tokens\n",
    "    filtered_tokens = filter(lambda x: len(x) > thresh, tokens)  # filter short tokens    \n",
    "    hashed_features = map(lambda x: hasher(x, n_buckets), filtered_tokens)\n",
    "    return hashed_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\n",
      "\n",
      "tokens: ['switchfoot', 'awww', \"that's\", 'bummer', 'you', 'shoulda', 'got', 'david', 'carr', 'of', 'third', 'day', 'to', 'do', 'it']\n",
      "\n",
      "hashed tokens: [6694, 9791, 9451, 8546, 4642, 7894, 3896, 4388, 9566, 7129, 9991]\n"
     ]
    }
   ],
   "source": [
    "print('text:', test_text, end='\\n'*2)\n",
    "print('tokens:', list(tokenizer(test_text)), end='\\n'*2)\n",
    "print('hashed tokens:', list(hashed_tokenizer(test_text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Построим TfIdfVectorizer над хешированными фичами\n",
    "Это как раз то, что делает под капотом sklearn.HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 47.7 s, sys: 455 ms, total: 48.2 s\n",
      "Wall time: 49.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "hashing_vectorizer = TfidfVectorizer(tokenizer=hashed_tokenizer)\n",
    "X_train_hashvect = hashing_vectorizer.fit_transform(X_train)\n",
    "X_test_hashvect = hashing_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1280000, 10000), (320000, 10000))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_hashvect.shape, X_test_hashvect.shape  # 10k новых признаков "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.762\n",
      "Recall: 0.785\n",
      "F1: 0.773\n",
      "CPU times: user 27.5 s, sys: 324 ms, total: 27.8 s\n",
      "Wall time: 29.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf.fit(X_train_hashvect, y_train)\n",
    "predicted = clf.predict(X_test_hashvect)\n",
    "\n",
    "print(\"Precision: %.3f\" % precision_score(y_test, predicted))\n",
    "print(\"Recall: %.3f\" % recall_score(y_test, predicted))\n",
    "print(\"F1: %.3f\" % f1_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1=0.773 для 10к признаков\n",
    "\n",
    "F1=0.798 - для 560к признаков\n",
    "\n",
    "Не особо заморачиваясь, снизили количество фичей в 50+ раз, и потеряли в качестве всего около ~3%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## А теперь проверим, как работает родной HashingVectorizer из sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.4 s, sys: 161 ms, total: 13.6 s\n",
      "Wall time: 13.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "hashing_vectorizer_sklearn = HashingVectorizer(n_features=10000)\n",
    "X_train_sklearn = hashing_vectorizer_sklearn.fit_transform(X_train)\n",
    "X_test_sklearn = hashing_vectorizer_sklearn.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "заметно быстрее, чем наш самопальный"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.771\n",
      "Recall: 0.783\n",
      "F1: 0.777\n",
      "CPU times: user 28.3 s, sys: 287 ms, total: 28.6 s\n",
      "Wall time: 29.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf.fit(X_train_sklearn, y_train)\n",
    "predicted = clf.predict(X_test_sklearn)\n",
    "\n",
    "print(\"Precision: %.3f\" % precision_score(y_test, predicted))\n",
    "print(\"Recall: %.3f\" % recall_score(y_test, predicted))\n",
    "print(\"F1: %.3f\" % f1_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "и чуть лучше по качеству"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "инструкции для установки фасттекста [тут](https://github.com/facebookresearch/fastText/tree/master/python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем тот же датасет, что и ранее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# подготовим файлы с датасетами для фасттекста\n",
    "dataset[\"label_ft\"] = dataset[\"sentiment\"].apply(lambda x: \"__label__\" + str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.concat([X_train.apply(lambda x: re.sub('\\t', '', x)), \n",
    "                           y_train.apply(lambda x: '__label__' + str(x))], axis=1)\n",
    "\n",
    "test_dataset = pd.concat([X_test.apply(lambda x: re.sub('\\t', '', x)), \n",
    "                          y_test.apply(lambda x: '__label__' + str(x))], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "сохраним в формате 'текст \\t метка'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.to_csv(\"train_fasttext.txt\", sep='\\t', header=None, index=None)\n",
    "test_dataset.to_csv(\"test_fasttext.txt\", sep='\\t', header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@jbtaylor WIth ya. &quot;I'd like a Palm Pre, Touchstone charger. ReadyNow? Yes, that sounds good. But is my beer ready now?'  #prelaunch\t__label__1\r\n",
      "felt the earthquake this afternoon, it seems to be a , but  at the epicenter \t__label__1\r\n"
     ]
    }
   ],
   "source": [
    "! head -2 train_fasttext.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "640506 __label__0\n",
      "639494 __label__1\n"
     ]
    }
   ],
   "source": [
    "!cut -f2 train_fasttext.txt | sort | uniq -c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Запустим обучение фасттекста на необработанном датасете"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(N, p, r):\n",
    "    print(\"Precision\\t{:.3f}\".format(p))\n",
    "    print(\"Recall\\t{:.3f}\".format(r))\n",
    "    print(\"F1\\t{:.3f}\".format(2*p*r/(p+r)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 56s, sys: 1.32 s, total: 1min 57s\n",
      "Wall time: 45.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_big = fasttext.train_supervised(\n",
    "    input=\"train_fasttext.txt\",\n",
    "    minCount=5,  # отсеиваем редкие токены\n",
    "    minn=3, maxn=5,  # диапазон для символьных нграмм\n",
    "    wordNgrams=2,  # используем словесные нграммы размера 2\n",
    "    dim=25  # размер вектора\n",
    ") # логи обучения пишутся в терминале"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_big.save_model(\"model_big.ft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--  1 d.parpulov  staff   203M 29 ноя 11:16 model_big.ft\r\n"
     ]
    }
   ],
   "source": [
    "ls -lh | grep model_big.ft  # 203 Mб моделька - не слабо!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision\t0.819\n",
      "Recall\t0.819\n",
      "F1\t0.819\n"
     ]
    }
   ],
   "source": [
    "print_results(*model_big.test('test_fasttext.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "и сразу из коробки получили качество, лучше чем до этого на 2-3%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но модель весит многовато. Давайте попробуем ужать количество бакетов для хеширования нграмм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 31s, sys: 1.05 s, total: 1min 32s\n",
      "Wall time: 35.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_small = fasttext.train_supervised(\n",
    "    input=\"train_fasttext.txt\",\n",
    "    minCount=5,  # отсеиваем редкие токены\n",
    "    minn=3, maxn=5,  # диапазон для символьных нграмм\n",
    "    wordNgrams=2,  # используем словесные нграммы размеры 2\n",
    "    dim=25, # размер вектора\n",
    "    bucket=200000, # количество бакетов для хеширования\n",
    ") # логи обучения пишутся в терминале"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_small.save_model(\"model_smaller.ft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--  1 d.parpulov  staff    32M 29 ноя 11:21 model_smaller.ft\r\n"
     ]
    }
   ],
   "source": [
    "ls -lh | grep model_smaller  # 32M моделька - уже лучше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision\t0.813\n",
      "Recall\t0.813\n",
      "F1\t0.813\n"
     ]
    }
   ],
   "source": [
    "print_results(*model_small.test('test_fasttext.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что модель стала сильно меньше, но а качество просело на полпроцента"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У фасттекста есть замечательная фича квантизации, которая позволяет сжимать модели с минимальной потерей качества\n",
    "\n",
    "Проверим, как она будет работать на меньшей модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_small.quantize(input=\"train_fasttext.txt\", retrain=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_small.save_model(\"model_compressed.ftz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--  1 d.parpulov  staff   5,8M 29 ноя 11:22 model_compressed.ftz\r\n"
     ]
    }
   ],
   "source": [
    "ls -lh | grep model_compressed  # итоговая модель весит всего 5,8 Мб"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision\t0.813\n",
      "Recall\t0.813\n",
      "F1\t0.813\n"
     ]
    }
   ],
   "source": [
    "print_results(*model_small.test('test_fasttext.txt'))  # а качество показывает такое же, как и непожатая модель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Посмотрим методы модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__0',), array([0.81586701]))"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# предскажем метку для текста\n",
    "model_small.predict(\"i hate everything about you!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00155861,  0.00076249, -0.00773714, -0.00307127,  0.00364631,\n",
       "       -0.00418105,  0.00483813, -0.00204994,  0.00718853,  0.01275729,\n",
       "        0.00109148,  0.00380854, -0.01695282, -0.00075939, -0.00462876,\n",
       "        0.00279449,  0.01129375,  0.00941746,  0.01609236,  0.0181504 ,\n",
       "       -0.02060649, -0.01163181, -0.01096021, -0.00530058, -0.00763473],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# получим вектор слова\n",
    "model_small.get_word_vector('man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.3495326e-05,  3.3990247e-03, -5.5876030e-03, -3.6635119e-03,\n",
       "        3.6372214e-03, -1.6306896e-03, -9.2436257e-04, -1.6920323e-03,\n",
       "        1.0075700e-02,  9.9754510e-03,  1.7363178e-03,  1.0817442e-03,\n",
       "       -1.3282857e-02,  3.0627372e-03, -3.2718773e-03,  4.3372135e-03,\n",
       "        4.2888047e-03,  4.3199151e-03,  1.3004756e-02,  1.6394349e-02,\n",
       "       -8.6986041e-03, -3.5347007e-03, -4.7837254e-03, -4.4796113e-03,\n",
       "       -2.4813798e-03], dtype=float32)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# получим вектор предложения\n",
    "model_small.get_sentence_vector(\"i hate everything about you!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.97241944]], dtype=float32)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# проверим, насколько похожи по контенту два предложения на основе косинусного расстояния между эмбедингами \n",
    "a = model_small.get_sentence_vector(\"I feel lousy\")\n",
    "b = model_small.get_sentence_vector(\"i hate everything about you!\")\n",
    "cosine_similarity([a],[b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.97110647]], dtype=float32)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = model_small.get_sentence_vector(\"i like that awesome song!\")\n",
    "b = model_small.get_sentence_vector(\"i hate everything about you!\")\n",
    "cosine_similarity([a],[b])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
